{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lance-Spark Getting Started\n",
    "\n",
    "This notebook demonstrates how to use Lance with Apache Spark for reading and writing Lance datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "The Spark session is already configured with the Lance catalog in the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "\n",
    "# Get the existing Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        # Directory namespace\n",
    "        .config(\"spark.sql.catalog.lance_dir\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.impl\", \"dir\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.root\", \"s3://lance-warehouse/dir_ns\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.storage.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.storage.aws_allow_http\", \"true\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.storage.access_key_id\", \"admin\")\n",
    "        .config(\"spark.sql.catalog.lance_dir.storage.secret_access_key\", \"password\")\n",
    "        # Glue namespace\n",
    "        .config(\"spark.sql.catalog.lance_glue\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.impl\", \"glue\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.root\", \"s3://lance-warehouse/glue_ns\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.access_key_id\", \"xyz\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.secret_access_key\", \"abc\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.region\", \"us-east-1\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.storage.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.storage.aws_allow_http\", \"true\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.storage.access_key_id\", \"admin\")\n",
    "        .config(\"spark.sql.catalog.lance_glue.storage.secret_access_key\", \"password\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Enable loading the Spark catalog\n",
    "spark.sql(\"set spark.sql.defaultCatalog=lance_glue\")\n",
    "spark.sql(\"use default\")\n",
    "\n",
    "# Verify Lance catalog is configured\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\", 75000),\n",
    "    (2, \"Bob\", 30, \"Marketing\", 65000),\n",
    "    (3, \"Charlie\", 35, \"Sales\", 70000),\n",
    "    (4, \"Diana\", 28, \"Engineering\", 80000),\n",
    "    (5, \"Eve\", 32, \"HR\", 60000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE employees (id INT, name STRING, age INT, department STRING, salary INT)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Describe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE TABLE EXTENDED employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"employees\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Query Lance Table using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the Lance table using SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           COUNT(*) as employee_count,\n",
    "           AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Append More Data to Lance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new data to append\n",
    "new_data = [\n",
    "    (6, \"Frank\", 29, \"Engineering\", 77000),\n",
    "    (7, \"Grace\", 31, \"Marketing\", 68000)\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Append to the Lance table\n",
    "new_df.writeTo(\"employees\").append()\n",
    "\n",
    "# Verify the append\n",
    "spark.sql(\"SELECT * FROM employees ORDER BY id DESC LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
