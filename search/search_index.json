{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Apache Spark Connector for Lance allows Apache Spark to efficiently read datasets stored in Lance format.</p> <p>Lance is a modern columnar data format optimized for machine learning workflows and datasets, supporting distributed, parallel scans, and optimizations such as column and filter pushdown to improve performance. Additionally, Lance provides high-performance random access that is 100 times faster than Parquet without sacrificing scan performance.</p> <p>By using the Apache Spark Connector for Lance, you can leverage Apache Spark's powerful data processing, SQL querying, and machine learning training capabilities on the AI data lake powered by Lance.</p>"},{"location":"#features","title":"Features","text":"<p>The connector is built using the Spark DatasourceV2 (DSv2) API. Please check this presentation to learn more about DSv2 features. Specifically, you can use the Apache Spark Connector for Lance to:</p> <ul> <li>Read &amp; Write Lance Datasets: Seamlessly read and write datasets stored in the Lance format using Spark.</li> <li>Distributed, Parallel Scans: Leverage Spark's distributed computing capabilities to perform parallel scans on Lance datasets.</li> <li>Column and Filter Pushdown: Optimize query performance by pushing down column selections and filters to the data source.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#start-session","title":"Start Session","text":"PySparkSpark Shell (Scala) <pre><code>pyspark --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1\n</code></pre> <pre><code>spark-shell --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1\n</code></pre>"},{"location":"#prepare-data","title":"Prepare Data","text":"PythonScala <pre><code>from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"name\", StringType(), False),\n    StructField(\"score\", FloatType(), False),\n])\n\ndata = [\n    (1, \"Alice\", 85.5),\n    (2, \"Bob\", 92.0),\n    (3, \"Carol\", 78.0),\n]\n\ndf = spark.createDataFrame(data, schema=schema)\n</code></pre> <pre><code>import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nval schema = StructType(Seq(\n  StructField(\"id\", IntegerType, nullable = false),\n  StructField(\"name\", StringType, nullable = false),\n  StructField(\"score\", FloatType, nullable = false)\n))\n\nval data = Seq(\n  Row(1, \"Alice\", 85.5f),\n  Row(2, \"Bob\", 92.0f),\n  Row(3, \"Carol\", 78.0f)\n)\n\nval rdd = spark.sparkContext.parallelize(data)\nval df = spark.createDataFrame(rdd, schema)\n</code></pre>"},{"location":"#simple-write","title":"Simple Write","text":"PythonScala <pre><code>(df.write\n    .format(\"lance\")\n    .mode(\"overwrite\")\n    .save(\"/tmp/manual_users.lance\"))\n</code></pre> <pre><code>df.write.\n    format(\"lance\").\n    mode(\"overwrite\").\n    save(\"/tmp/manual_users.lance\")\n</code></pre>"},{"location":"#simple-read","title":"Simple Read","text":"PythonScala <pre><code>(spark.read\n    .format(\"lance\")\n    .load(\"/tmp/manual_users.lance\")\n    .show())\n</code></pre> <pre><code>spark.read.\n    format(\"lance\").\n    load(\"/tmp/manual_users.lance\").\n    show()\n</code></pre>"},{"location":"dev-guide/","title":"Development Guide","text":""},{"location":"dev-guide/#lance-java-sdk-dependency","title":"Lance Java SDK Dependency","text":"<p>This package is dependent on the Lance Java SDK and Lance Namespace Java Modules. You need to build those repositories locally first before building this repository. If your have changes affecting those repositories, the PR in <code>lancedb/lance-spark</code> will only pass CI after the PRs in <code>lancedb/lance</code> and <code>lance/lance-namespace</code> are merged.</p>"},{"location":"dev-guide/#build-commands","title":"Build Commands","text":"<p>This connector is built using Maven. To build everything:</p> <pre><code>./mvnw clean install\n</code></pre> <p>To build everything without running tests:</p> <pre><code>./mvnw clean install -DskipTests\n</code></pre>"},{"location":"dev-guide/#multi-version-support","title":"Multi-Version Support","text":"<p>We offer the following build profiles for you to switch among different build versions:</p> <ul> <li>scala-2.12</li> <li>scala-2.13</li> <li>spark-3.4</li> <li>spark-3.5</li> </ul> <p>For example, to use Scala 2.13:</p> <pre><code>./mvnw clean install -Pscala-2.13\n</code></pre> <p>To build a specific version like Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4\n</code></pre> <p>To build only Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4 -pl lance-spark-3.4 -am\n</code></pre> <p>Use the <code>shade-jar</code> profile to create the jar with all dependencies for Spark 3.4:</p> <pre><code>./mvnw clean install -Pspark-3.4 -Pshade-jar -pl lance-spark-3.4 -am\n</code></pre>"},{"location":"dev-guide/#styling-guide","title":"Styling Guide","text":"<p>We use checkstyle and spotless to lint the code.</p> <p>To verify checkstyle:</p> <pre><code>./mvnw checkstyle:check\n</code></pre> <p>To verify spotless:</p> <pre><code>./mvnw spotless:check\n</code></pre> <p>To apply spotless changes:</p> <pre><code>./mvnw spotless:apply\n</code></pre>"},{"location":"dev-guide/#documentation-website","title":"Documentation Website","text":""},{"location":"dev-guide/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. The easiest way to setup is to create a Python virtual environment and install the necessary dependencies:</p> <pre><code>python3 -m venv .env\nsource .env/bin/activate\npip install mkdocs-material\npip install mkdocs-awesome-pages-plugin\n</code></pre> <p>Then you can start the server at <code>http://localhost:8000/lance-spark</code> by:</p> <pre><code>source .env/bin/activate\nmkdocs serve\n</code></pre>"},{"location":"dev-guide/#contents","title":"Contents","text":"<p>In general, we push most of the contents in the website as the single source of truth. The welcome page is the same as the README of the GitHub repository. If you edit one of them, please make sure to update the other document.</p>"},{"location":"user-guide/config/","title":"Config","text":""},{"location":"user-guide/config/#spark-catalog-configuration","title":"Spark Catalog Configuration","text":"<p>You should configure Spark with <code>com.lancedb.lance.spark.LanceCatalog</code> DSv2 catalog:</p> PySpark <pre><code>pyspark \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceCatalog\n</code></pre> Spark Shell (Scala) <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceCatalog\n</code></pre> Spark Submit <pre><code>spark-submit \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceCatalog\n</code></pre> Spark SQL <pre><code>spark-sql \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.1 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceCatalog\n</code></pre>"},{"location":"user-guide/config/#spark-dataframe-options","title":"Spark DataFrame Options","text":"Option Type Required? Default Description <code>db</code> String \u2705 Path to the Lance database directory <code>dataset</code> String \u2705 Name of the Lance dataset"},{"location":"user-guide/config/#example","title":"Example","text":"<pre><code>Dataset&lt;Row&gt; df = spark.read()\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/lance/database\")\n    .option(\"dataset\", \"my_dataset\")\n    .load();\n</code></pre>"},{"location":"user-guide/install/","title":"Install","text":""},{"location":"user-guide/install/#maven-central-packages","title":"Maven Central Packages","text":"<p>The connector packages are published to Maven Central under the <code>com.lancedb</code> namespace. Choose the appropriate artifact based on your use case:</p>"},{"location":"user-guide/install/#available-artifacts","title":"Available Artifacts","text":"Artifact Type Name Pattern Description Example Base Jar <code>lance-spark-base_&lt;scala_version&gt;</code> Jar with logic shared by different versions of Spark Lance connectors. lance-spark-base_2.12 Lean Jar <code>lance-spark-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with only the Spark Lance connector logic lance-spark-3.5_2.12 Bundled Jar <code>lance-spark-bundle-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with all necessary non-Spark dependencies lance-spark-bundle-3.5_2.12"},{"location":"user-guide/install/#choosing-the-right-artifact","title":"Choosing the Right Artifact","text":"<ul> <li>Bundled Jar: Recommended for most users. Use this if you want to quickly get started or use the connector in a Spark shell/notebook environment.</li> <li>Lean Jar: Use this if you're building a custom Spark application and want to manage and bundle dependencies yourself.</li> <li>Base Jar: Internal use only. Use this if you would like to build a custom Spark Lance connector with a different Spark version.</li> </ul>"},{"location":"user-guide/install/#dependency-configuration","title":"Dependency Configuration","text":""},{"location":"user-guide/install/#in-spark-application-code","title":"In Spark Application Code","text":"<p>Typically, you use the bundled jar in your Spark application as a provided (compile only) dependency. The actual jar is supplied to the Spark cluster separately. If you want to also include the bundled jar in your own bundle, remove the provided (compile only) annotation.</p> MavenGradlesbt <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.lancedb.lance&lt;/groupId&gt;\n    &lt;artifactId&gt;lance-spark-bundle-3.5_2.12&lt;/artifactId&gt;\n    &lt;version&gt;0.0.1&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>dependencies {\n    compileOnly 'com.lancedb.lance:lance-spark-bundle-3.5_2.12:0.0.1'\n}\n</code></pre> <pre><code>libraryDependencies += \"com.lancedb.lance\" %% \"lance-spark-bundle-3.5_2.12\" % \"0.0.1\" % \"provided\"\n</code></pre>"},{"location":"user-guide/install/#in-spark-cluster","title":"In Spark Cluster","text":"<p>You can either download the bundled jar dependency from Maven and add it to your Spark classpath, or supply the dependency dynamically to a Spark cluster through <code>--packages</code> flag. See Spark Catalog Configuration for more details.</p>"},{"location":"user-guide/install/#requirements","title":"Requirements","text":""},{"location":"user-guide/install/#java","title":"Java","text":"Java Version Support Status Notes Java 8 \u2705 Supported Minimum required version Java 11 \u2705 Supported Recommended for production Java 17 \u2705 Supported Latest LTS version Java 21+ \u26a0\ufe0f Untested May work but not officially tested"},{"location":"user-guide/install/#scala","title":"Scala","text":"Scala Version Support Status Notes Scala 2.12 \u2705 Supported Required Scala 2.13 \ud83d\udea7 In Progress Support planned for future releases Scala 3.x \u274c Not Supported Not currently planned"},{"location":"user-guide/install/#apache-spark","title":"Apache Spark","text":"Spark Version Support Status Notes Spark 3.5 \u2705 Supported Primary supported version Spark 3.4 \u2705 Supported Requires manual building, Maven publication work in progress Spark 3.1, 3.2, 3.3 \u26a0\ufe0f Untested May work but not officially tested Spark 4.0 \ud83d\udea7 In Progress Support planned for future releases Spark 2.x \u274c Not Supported"},{"location":"user-guide/install/#operating-system","title":"Operating System","text":"Operating System Architecture Support Status Notes Linux x86_64 \u2705 Supported Linux ARM64 \u2705 Supported Including Apple Silicon via Rosetta macOS x86_64 \u2705 Supported Intel-based Macs macOS ARM64 \u2705 Supported Apple Silicon (M1/M2/M3) Windows x86_64 \ud83d\udea7 In Progress Support planned for future releases"},{"location":"user-guide/read/","title":"Reading Lance Datasets","text":""},{"location":"user-guide/read/#basic-reading","title":"Basic Reading","text":"PythonScalaJava <pre><code>df = (spark.read\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/lance/database\")\n    .option(\"dataset\", \"my_dataset\")\n    .load())\n</code></pre> <pre><code>val df = spark.read.\n    format(\"lance\").\n    option(\"db\", \"/path/to/lance/database\").\n    option(\"dataset\", \"my_dataset\").\n    load()\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = spark.read()\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/lance/database\")\n    .option(\"dataset\", \"my_dataset\")\n    .load();\n</code></pre>"},{"location":"user-guide/read/#column-selection","title":"Column Selection","text":"<p>Lance is a columnar format. You can specify which columns to read to improve performance:</p> PythonScalaJava <pre><code>df = (spark.read\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/lance/database\")\n    .option(\"dataset\", \"my_dataset\")\n    .load()\n    .select(\"id\", \"name\", \"age\"))\n</code></pre> <pre><code>val df = spark.read.\n    format(\"lance\").\n    option(\"db\", \"/path/to/lance/database\").\n    option(\"dataset\", \"my_dataset\").\n    load().\n    select(\"id\", \"name\", \"age\")\n</code></pre> <pre><code>Dataset&lt;Row&gt; df = spark.read()\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/lance/database\")\n    .option(\"dataset\", \"my_dataset\")\n    .load()\n    .select(\"id\", \"name\", \"age\");\n</code></pre>"},{"location":"user-guide/read/#filters","title":"Filters","text":"<p>You can apply filters to a read. The filter is pushed down to reduce the amount of data read:</p> PythonScalaJava <pre><code>from pyspark.sql.functions import col\n\nfiltered = (spark.read\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/database\")\n    .option(\"dataset\", \"users\")\n    .load()\n    .filter(\n        col(\"age\").between(25, 65) &amp;\n        col(\"department\") == \"Engineering\" &amp;\n        col(\"is_active\") == True\n    ))\n</code></pre> <pre><code>import org.apache.spark.sql.functions.col\n\nval filtered = spark.read.\n    format(\"lance\").\n    option(\"db\", \"/path/to/database\").\n    option(\"dataset\", \"users\").\n    load().\n    filter(\n        col(\"age\").between(25, 65) &amp;&amp;\n        col(\"department\") === \"Engineering\" &amp;&amp;\n        col(\"is_active\") === true\n    )\n</code></pre> <pre><code>Dataset&lt;Row&gt; filtered = spark.read()\n    .format(\"lance\")\n    .option(\"db\", \"/path/to/database\")\n    .option(\"dataset\", \"users\")\n    .load()\n    .filter(\"age BETWEEN 25 AND 65 AND department = 'Engineering' AND is_active = true\");\n</code></pre>"},{"location":"user-guide/write/","title":"Writing Lance Datasets","text":""},{"location":"user-guide/write/#basic-writing","title":"Basic Writing","text":"PythonScalaJava <pre><code>(df.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/lance/database/my_dataset\")\n    .save())\n</code></pre> <pre><code>df.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/lance/database/my_dataset\").\n    save()\n</code></pre> <pre><code>df.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/lance/database/my_dataset\")\n    .save();\n</code></pre> <p>Alternatively, you can specify the path directly in the <code>save()</code> method:</p> PythonScalaJava <pre><code>(df.write\n    .format(\"lance\")\n    .save(\"/path/to/lance/database/my_dataset\"))\n</code></pre> <pre><code>df.write.\n    format(\"lance\").\n    save(\"/path/to/lance/database/my_dataset\")\n</code></pre> <pre><code>df.write()\n    .format(\"lance\")\n    .save(\"/path/to/lance/database/my_dataset\");\n</code></pre>"},{"location":"user-guide/write/#write-modes","title":"Write Modes","text":""},{"location":"user-guide/write/#create","title":"Create","text":"<p>By default, writing to a dataset at a specific path means creating the dataset:</p> PythonScalaJava <pre><code># First write - succeeds\n(testData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save())\n\n# Second write - throws TableAlreadyExistsException\n(testData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save())\n</code></pre> <pre><code>// First write - succeeds\ntestData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    save()\n\n// Second write - throws TableAlreadyExistsException\ntestData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    save()\n</code></pre> <pre><code>// First write - succeeds\ntestData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save();\n\n// Second write - throws TableAlreadyExistsException\ntestData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save();\n</code></pre>"},{"location":"user-guide/write/#append","title":"Append","text":"<p>Add new data to an existing dataset:</p> PythonScalaJava <pre><code># Create initial dataset\n(testData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save())\n\n# Append more data\n(moreData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .mode(\"append\")\n    .save())\n</code></pre> <pre><code>// Create initial dataset\ntestData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    save()\n\n// Append more data\nmoreData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    mode(\"append\").\n    save()\n</code></pre> <pre><code>// Create initial dataset\ntestData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save();\n\n// Append more data\nmoreData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .mode(\"append\")\n    .save();\n</code></pre>"},{"location":"user-guide/write/#overwrite","title":"Overwrite","text":"<p>Replace the entire dataset with new data:</p> PythonScalaJava <pre><code># Create initial dataset\n(initialData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save())\n\n# Completely replace the dataset\n(newData.write\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .mode(\"overwrite\")\n    .save())\n</code></pre> <pre><code>// Create initial dataset\ninitialData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    save()\n\n// Completely replace the dataset\nnewData.write.\n    format(\"lance\").\n    option(\"dataset_uri\", \"/path/to/database/users\").\n    mode(\"overwrite\").\n    save()\n</code></pre> <pre><code>// Create initial dataset\ninitialData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .save();\n\n// Completely replace the dataset\nnewData.write()\n    .format(\"lance\")\n    .option(\"dataset_uri\", \"/path/to/database/users\")\n    .mode(\"overwrite\")\n    .save();\n</code></pre>"}]}